# -*- coding: utf-8 -*-
"""Копия_BertModel_sentiment_analysisCOPY_(summary_last).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t-Gw5LvQohfwzDt0SBWyQHfc5eA-M1sN
"""

from google.colab import drive
import json
import zipfile

drive.mount('/content/drive')

!pip install transformers

# Импорт необходимых библиотек
import re
from tqdm.notebook import tqdm
from typing import Union, List
from IPython.display import clear_output

import torch
import torch.nn as nn

from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split
from transformers import pipeline, AutoTokenizer, AutoModel, BertTokenizer, BertModel

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
plt.style.use('dark_background')

# Set GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#загрузка датасета
df_full = pd.read_csv('/content/drive/MyDrive/Colabs/datasets/MovieReviewTrainingDatabase.csv', sep=',')
df = df_full.sample(10000)

df

df.isnull().sum()

df.dtypes

# названия категорий которые будем оставлять в датафрейме
CATEGORY_NAMES = ['Positive', 'Negative']

# служебные словари
INDEX_TO_CLASS = dict(enumerate(CATEGORY_NAMES))
CLASS_TO_INDEX = {value: key for key, value in INDEX_TO_CLASS.items()}
CLASS_TO_INDEX

# оставить в датасете только выбранные категории
df = df[df.sentiment.isin(CATEGORY_NAMES)]
df.shape

type(CLASS_TO_INDEX)

df

# удалить дубликаты
df = df.drop_duplicates(subset=['review'])
df.shape

df

def preprocess_text(text):
    # привести в нижнему регистру
    text = text.lower()
    # удаление 0 или более цифр и знаков переноса строки в конце предложения
    text = re.sub('^\d*\n*|\n*\d*$', '', text)
    return text

#df['review'] = df.review - форма записи.
df.review = df.review.apply(preprocess_text)
df.head()

df['review']

"""Tokenize Data"""

from transformers import AutoTokenizer

model = BertModel.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# взять пример текста текст из датафрейма
text = df[1:2].review.values.item()
text, len(text)

# срез для подачи в токенайзер
text[0:50]

# токенизация
tokenizer.tokenize(text[0:50])

# токенизация и сопоставление индексов токенов с добавлением служебных токенов
encoded = tokenizer.encode('Hello Everyone!', add_special_tokens=True)
encoded

# из индексов токенов в слова
tokenizer.decode(encoded)

max_calc_len = 0
for sent in df.review:
    input_ids = tokenizer.encode(sent, add_special_tokens=True)
    max_calc_len = max(max_calc_len, len(input_ids))

print(f'Максимальная длина токенизированного текста: {max_calc_len}')

"""**Кодирование текста для подачи в модель**  
Пройти по каждому тексту - предложению в цикле и с помощью `tokenizer.encode_plus` сделать следующее:
1. Токенизировать предложение.
2. Добаввить токен `[CLS]` в начало, добаввить токен `[SEP]` в конец предложения.
3. Сопоставвить токены с их индексами в словаре.
4. Дополнить или сократить предложение до максимальной длины `max_length`.
5. Создавить маски внимания для токенов паддинга `[PAD]`.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # макс длина текста в токенах
# max_len = 128
# 
# # списки для индексов токенов, масок внимания и тагрет индексов категорий текста
# input_ids = []
# attention_masks = []
# target_ids = []
# 
# # итерация по строкам всех столбцов датафрейма
# for row in df.itertuples():
#     # текст из столбца review
#     text = row.review
#     encoded_dict = tokenizer(text,  # текст строка которую кодируем
#                         add_special_tokens=True,  # добавить '[CLS]' и '[SEP]' токены
#                         max_length=max_len,  # параметр максимальной длины текста
#                         padding='max_length',  # делать падинг до макс длины
#                         truncation=True,  # если длина больше max_length то отрезать лишнее
#                         return_attention_mask=True,  # делать ли маску внимания
#                         return_tensors='pt',  # формат возвращаемого тензора
#                         # return_token_type_ids=False,
#                    )
# 
#     # обновить списки
#     input_ids.append(encoded_dict['input_ids'])
#     attention_masks.append(encoded_dict['attention_mask'])
# 
#     target_id = CLASS_TO_INDEX[row.sentiment]
#     target_ids.append(target_id)
# 
# # преобразовать списки в тензоры
# input_ids = torch.cat(input_ids, dim=0)
# attention_masks = torch.cat(attention_masks, dim=0)
# target_ids = torch.tensor(target_ids)
# 
# # 8315 предложений длиной 128, столько же масок внимания и 8315 тагрет индексов категорий текста
# input_ids.shape, attention_masks.shape, target_ids.shape

print('Оригинальный текст: ', text)
print('Токенизированный текст:', encoded_dict['input_ids'])
print('Класс текста:', INDEX_TO_CLASS[target_id])

# датасет из тензоров
dataset = TensorDataset(input_ids, attention_masks, target_ids)

# разделение на датасеты для обучения и валидации
val_size = 0.2
train_dataset, val_dataset = random_split(dataset, [1 - val_size, val_size])

# кол-во предложений в каждом датасете
len(train_dataset), len(val_dataset)

BATCH_SIZE = 32
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)  # pin_memory=True
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)  # pin_memory=True

input_ids, attention_mask, target_ids = next(iter(train_loader))
input_ids.shape, attention_mask.shape, target_ids.shape

from transformers import BertTokenizer, BertForSequenceClassification

# инициализация модели с необходимым кол-вом классов
num_labels = len(CLASS_TO_INDEX)
model_name = 'google-bert/bert-base-uncased'
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

# архитектура модели - на выходе есть случайно инициализированный линейный слой classifier который надо обучить
model

"""## Forward pass"""

# получение батча из даталоадера
input_ids, attention_mask, target_ids = next(iter(train_loader))
input_ids.shape, attention_mask.shape, target_ids.shape

# берт подобные модели на вход обычно принимают индексы токенов и маски внимания
outputs = model(input_ids=input_ids, attention_mask=attention_mask)

# так как на выходе уже есть слой для классификации
outputs.keys(), outputs.logits.shape

# расчет ошибки снаружи
loss_fn = nn.CrossEntropyLoss()
loss = loss_fn(outputs.logits, target_ids)
loss

# предсказанные классы категории текста для расчета accuracy
pred_tokens_ids = outputs.logits.argmax(-1)
pred_tokens_ids, pred_tokens_ids.shape

"""## Train

Инициализация модели, оптимизатора, шедулера, функции ошибки
"""

# get_linear_schedule_with_warmup для изменения скорости обучения оптимизатора
from transformers import get_linear_schedule_with_warmup

torch.manual_seed(111)
# кол-во классов
num_labels = len(CLASS_TO_INDEX)
# название модели
model_path = 'google-bert/bert-base-uncased'
# инициализация модели
model = BertForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)

# оптимизатор с маленькой скоростью потому что трансформер и потому что шедулер будет ее увеличивать
LR = 0.000_02
optimizer = torch.optim.AdamW(model.parameters(), lr=LR)

# кол-во эпох обучения
EPOCHS = 5
# расчет общего кол-ва шагов обучения для шедулера
total_steps = len(train_loader) * EPOCHS

# инициализация планировщика - шедулера
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=len(train_loader) * total_steps)

# функция ошибки
loss_fn = torch.nn.CrossEntropyLoss()
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(DEVICE)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # обучалось на T4 в Colab
# torch.manual_seed(111)
# train_losses, val_losses, train_accs, val_accs = [], [], [], []
# 
# for epoch in range(EPOCHS):
#     # ========================= TRAIN ===============================
#     model.train()
# 
#     correct_predictions = 0
#     epoch_loss = 0
# 
#     for input_ids, attention_mask, targets in tqdm(train_loader, desc='Training', leave=False):
#         # переместить все тензоры на девайс
#         input_ids = input_ids.to(DEVICE)
#         attention_mask = attention_mask.to(DEVICE)
#         targets = targets.to(DEVICE)
# 
#         # передать в модель индексы токенов и макси внимания
#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)
# 
#         # расчет ошибки
#         loss = loss_fn(outputs.logits, targets)
#         epoch_loss += loss.item()
# 
#         # получение ответов модели для расчета accuracy и расчет accuracy
#         preds = torch.argmax(outputs.logits, dim=1)
#         correct_predictions += torch.sum(preds == targets).item()
# 
#         # обучение моедли
#         loss.backward()
#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
#         optimizer.step()
#         scheduler.step()
#         optimizer.zero_grad()
# 
#     # нормировка ошибки accuracy
#     train_loss = epoch_loss / len(train_loader)
#     train_acc = correct_predictions / len(train_loader.dataset)
# 
#     train_losses.append(train_loss)
#     train_accs.append(train_acc)
# 
# 
#     # ==================== VALIDATION ============================
#     model.eval()
# 
#     correct_predictions = 0
#     epoch_loss = 0
# 
#     for input_ids, attention_mask, targets in tqdm(val_loader, desc='Validation', leave=False):
#         input_ids = input_ids.to(DEVICE)
#         attention_mask = attention_mask.to(DEVICE)
#         targets = targets.to(DEVICE)
# 
#         with torch.inference_mode():
#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)
# 
#         preds = torch.argmax(outputs.logits, dim=1)
#         loss = loss_fn(outputs.logits, targets)
# 
#         epoch_loss += loss.item()
#         correct_predictions += torch.sum(preds == targets).item()
# 
#     val_loss = epoch_loss / len(train_loader)
#     val_acc = correct_predictions / len(val_loader.dataset)
# 
#     val_losses.append(val_loss)
#     val_accs.append(val_acc)
# 
# 
#     # ================ PLOT METRICS ===================================
#     clear_output(True)
#     plt.figure(figsize=(10, 4))
#     plt.subplot(1, 2, 1)
#     plt.title(f'Train loss: {train_losses[-1]:.2f}, Vals loss {val_losses[-1]:.2f}')
#     plt.plot(train_losses, label='Train loss')
#     plt.plot(val_losses, label='Val loss')
#     plt.legend()
# 
#     plt.subplot(1, 2, 2)
#     plt.title(f'Train acc: {train_accs[-1]:.2f}, Vals acc {val_accs[-1]:.2f}')
#     plt.plot(train_accs, label='Train accuracy')
#     plt.plot(val_accs, label='Val accuracy')
#     plt.legend()
# 
#     plt.show()

"""## Inference

Функция для предсказания класса текста моделью  
Токенизировать текст, добавить служебные токены, сопоставить индексы, переместить все тензоры на дейвайс, сделать предсказание  
Теперь не обязательно добивать паддингами предложения до макс длины, однако если длина будет больше 512 (по умолчанию в Берте) то будет ошибка поэтому ограничим макс длину с помощью `truncation=True`
"""

@torch.inference_mode()
def predict_text(model, text):
    encoded_dict = tokenizer(text,  # текст строка которую кодируем
                        add_special_tokens=True,  # добавить '[CLS]' и '[SEP]' токены
                        truncation=True,  # если длина больше 512 то отрезать лишнее
                        return_attention_mask=True,  # делать ли маску внимания
                        return_tensors='pt',  # формат возвращаемого тензора
                        # return_token_type_ids=False,
                   )
    # переместить тензоры на девайс
    encoded_dict = {key: value.to(DEVICE) for key, value in encoded_dict.items()}
    outputs = model(**encoded_dict)
    pred_tokens_ids = outputs.logits.argmax(-1).item()
    print(f'Класс текста: {INDEX_TO_CLASS[pred_tokens_ids]}')

# перевести модель в режим оценки
model.eval();

text = 'YOUR APP IS NICE!'
predict_text(model, text)

text = 'VERY BAD APP!'
predict_text(model, text)

inference = predict_text(model, text)

# save_model_path_avi_test = 'bert-base-uncased_avi_test'

# trainer.save_model(save_model_path)
# tokenizer.save_pretrained(save_model_path);

from huggingface_hub import notebook_login

# Login to HF Hub
notebook_login()

from transformers import Pipeline, AutoTokenizer

# Custom Inference Pipeline
class avitestPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):
        preprocess_kwargs = {}
        if "text" in kwargs:
            preprocess_kwargs["text"] = kwargs["text"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, text, **kwargs):
        textPre_processing = ProcessText()
        processed_description = textPre_processing(text)
        try:
            if type(processed_description) == str:
                tokenizer = AutoTokenizer.from_pretrained("avi30/avitoken_test")
                processed_description = str(processed_description)
                predToken = tokenizer.encode(processed_description, add_special_tokens=True)

                max_len = 155
                padded_predToken = np.array([predToken + [0]*(max_len-len(predToken))])
                predAttention_mask = np.where(padded_predToken != 0, 1, 0)

                input_idsPred = torch.tensor(padded_predToken)
                attention_maskPred = torch.tensor(predAttention_mask)

                return {"input_idsPred": input_idsPred, "attention_maskPred": attention_maskPred}
        except Exception as error:
            print("{}".format(str(error)))
            return -1

    def _forward(self, model_inputs):
        input_idsPred = model_inputs["input_idsPred"]
        attention_maskPred = model_inputs["attention_maskPred"]
        self.model = self.model.to("cuda")  # Ensure model is on CUDA if available

        with torch.no_grad():
            output = self.model(input_idsPred.to("cuda"), token_type_ids=None, attention_mask=attention_maskPred.to("cuda"))
        prediction = 1 if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 0

        return {"logits": "agri" if prediction == 1 else "non-agri"}

    def postprocess(self, model_outputs, **kwargs):
        return model_outputs["logits"]

# Commented out IPython magic to ensure Python compatibility.
# %%writefile avitest_custom_pipeline.py

# Commented out IPython magic to ensure Python compatibility.
# %%writefile avitest_custom_pipeline.py
# 
# from transformers import AutoTokenizer, BertForSequenceClassification, Pipeline
# from nltk.tokenize import word_tokenize
# from nltk.stem import WordNetLemmatizer
# from nltk.corpus import stopwords
# from nltk.corpus import wordnet
# import numpy as np
# import warnings
# import string
# import torch
# import nltk
# import re
# 
# # Download necessary NLTK packages
# nltk.download('averaged_perceptron_tagger')
# nltk.download("stopwords")
# nltk.download('wordnet')
# nltk.download('punkt')
# 
# # Supress warning
# warnings.filterwarnings('ignore')
# 
# # pre-processing modules
# class RemovePunctuation:
#     """
#     class to remove the corresponding punctuation from the list of punctuations
#     """
# 
#     def __init__(self):
#         """
#         :param empty: None
#         """
#         self.punctuation = string.punctuation
# 
#     def __call__(self, punctuations):
#         """
#         Apply the transformations above.
#         :param punctuation: take the single punctuation(in my case '?')
#         :return: transformed punctuation list, excluding the '?'
#         """
#         if type(punctuations) == str:
#             punctuations = list(punctuations)
#         for punctuation in punctuations:
#             self.punctuation = self.punctuation.translate(str.maketrans('', '', punctuation))
#         return self.punctuation
# 
# 
# # Accessing the remove_punctuation object
# remove_punctuation = RemovePunctuation()
# 
# 
# def get_wordnet_pos(tag):
#     if tag.startswith('J'):
#         return wordnet.ADJ
#     elif tag.startswith('V'):
#         return wordnet.VERB
#     elif tag.startswith('N'):
#         return wordnet.NOUN
#     elif tag.startswith('R'):
#         return wordnet.ADV
#     else:
#         return wordnet.NOUN  # Default to Noun if the part of speech is not recognized
# 
# 
# class ProcessText(object):
# 
#     @staticmethod
#     def remove_punctuation_text(text):
#         """custom function to remove the punctuation"""
#         res = (re.findall(r'\w+|[^\s\w]+', text))
#         name = []
#         for word in res:
#             clean_word = word.translate(str.maketrans('', '', remove_punctuation("")))
#             if clean_word != "":
#                 name.append(clean_word)
# 
#         return " ".join(name)
# 
#     @staticmethod
#     def remove_stopwords(text):
#         stop_words = set(stopwords.words('english'))
#         words = word_tokenize(text)
#         filtered_words = [word for word in words if word.lower() not in stop_words]
#         return ' '.join(filtered_words)
# 
#     @staticmethod
#     def lower_casing(text):
#         text_lower = text.lower()
# 
#         return text_lower
# 
# 
#     @staticmethod
#     def lemmatize_text(text):
#         lemmatizer = WordNetLemmatizer()
#         words = word_tokenize(text)
#         tagged_words = nltk.pos_tag(words)
#         lemmatized_words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in tagged_words]
#         return ' '.join(lemmatized_words)
# 
#     @staticmethod
#     def remove_duplicates_and_sort(text):
#         # Split the text into individual words
#         words = text.split()
# 
#         # Create a set to store unique words (which automatically removes duplicates)
#         unique_words = set(words)
# 
#         # Sort the unique words based on their original order in the text
#         sorted_unique_words = sorted(unique_words, key=lambda x: words.index(x))
# 
#         # Join the sorted unique words back into a string with space as separator
#         sorted_text = ' '.join(sorted_unique_words)
# 
#         return sorted_text
# 
#     @staticmethod
#     def remove_numbers(text):
#         # Use regex to replace all numbers with an empty string
#         cleaned_text = re.sub(r'\d+', '', text)
#         return cleaned_text
# 
#     @staticmethod
#     def include_words_with_len_greater_than_2(text):
#         # Split the text into words
#         words = text.split()
# 
#         # Filter out words with length greater than 2
#         filtered_words = [word for word in words if len(word) > 2]
# 
#         # Join the filtered words back into a text
#         cleaned_text = ' '.join(filtered_words)
# 
#         return cleaned_text
# 
#     def __call__(self, text):
#         # remove any punctuation
#         text = self.remove_punctuation_text(text)
# 
#         # Covert text into lower case
#         text = self.lower_casing(text)
# 
#         # Stopwords such as "is", "the", etc that coney no meaning are removed
#         text = self.remove_stopwords(text)
# 
#         # Lemmatization is done for converting words to their base or root form, considering their context and part of speech.
#         text = self.lemmatize_text(text)
# 
#         # Since words are independent to one another in our problem scenario we can sort the text by word and remove any kind of duplicacy
#         text = self.remove_duplicates_and_sort(text)
# 
#         cleaned_text = self.include_words_with_len_greater_than_2(self.remove_numbers(text))
# 
#         return cleaned_text
# 
# 
# # custom inference pipeline
# class avitestPipeline(Pipeline):
#     def _sanitize_parameters(self, **kwargs):
#         preprocess_kwargs = {}
#         if "text" in kwargs:
#             preprocess_kwargs["text"] = kwargs["text"]
#         return preprocess_kwargs, {}, {}
# 
#     def preprocess(self, text, **kwargs):
#         textPre_processing = ProcessText()
#         processed_description = textPre_processing(text)
#         try:
#             if type(processed_description) == str:
#                 tokenizer = AutoTokenizer.from_pretrained("avi30/avitoken_test")
#                 processed_description = str(processed_description)
#                 predToken = tokenizer.encode(processed_description, add_special_tokens=True)
# 
#                 max_len = 155
#                 padded_predToken = np.array([predToken + [0]*(max_len-len(predToken))])
#                 predAttention_mask = np.where(padded_predToken != 0, 1, 0)
# 
#                 input_idsPred = torch.tensor(padded_predToken)
#                 attention_maskPred = torch.tensor(predAttention_mask)
# 
#                 return {"input_idsPred": input_idsPred, "attention_maskPred": attention_maskPred}
#         except Exception as error:
#             print("{}".format(str(error)))
#             return -1
# 
#     def _forward(self, model_inputs):
#         input_idsPred = model_inputs["input_idsPred"]
#         attention_maskPred = model_inputs["attention_maskPred"]
#         self.model = self.model.to("cuda")  # Ensure model is on CUDA if available
# 
#         with torch.no_grad():
#             output = self.model(input_idsPred.to("cuda"), token_type_ids=None, attention_mask=attention_maskPred.to("cuda"))
#         prediction = 1 if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 0
# 
#         return {"logits": "agri" if prediction == 1 else "non-agri"}
# 
#     def postprocess(self, model_outputs, **kwargs):
#         return model_outputs["logits"]

from drive/avitest_custom_pipeline import avitestPipeline

from transformers import BertForSequenceClassification
from transformers.pipelines import PIPELINE_REGISTRY

# Register your custom pipeline
PIPELINE_REGISTRY.register_pipeline(
    "avi-classification",
    pipeline_class = avitestPipeline,
    pt_model = BertForSequenceClassification
)

avi_classifier = pipeline("avi-classification", model="avi30/avitoken_test")
avi_classifier("Very nice work!GUYS!")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile avi_pipeline.py
# import numpy as np
# 
# from transformers import Pipeline
# 
# 
# def softmax(outputs):
#     maxes = np.max(outputs, axis=-1, keepdims=True)
#     shifted_exp = np.exp(outputs - maxes)
#     return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)
# 
# 
# class aviClassificationPipeline(Pipeline):
#     def _sanitize_parameters(self, **kwargs):
#         preprocess_kwargs = {}
#         if "second_text" in kwargs:
#             preprocess_kwargs["second_text"] = kwargs["second_text"]
#         return preprocess_kwargs, {}, {}
# 
#     def preprocess(self, text, second_text=None):
#         return self.tokenizer(text, text_pair=second_text, return_tensors=self.framework)
# 
#     def _forward(self, model_inputs):
#         return self.model(**model_inputs)
# 
#     def postprocess(self, model_outputs):
#         logits = model_outputs.logits[0].numpy()
#         probabilities = softmax(logits)
# 
#         best_class = np.argmax(probabilities)
#         label = self.model.config.id2label[best_class]
#         score = probabilities[best_class].item()
#         logits = logits.tolist()
#         return {"label": label, "score": score, "logits": logits}

from avi_pipeline import aviClassificationPipeline
from transformers.pipelines import PIPELINE_REGISTRY
from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification

PIPELINE_REGISTRY.register_pipeline(
    "sentiment-analysis",
    pipeline_class=aviClassificationPipeline,
    pt_model=AutoModelForSequenceClassification,
    tf_model=TFAutoModelForSequenceClassification,
)

from transformers import pipeline

classifier = pipeline("sentiment-analysis", model="avi30/avitoken_test")

classifier.push_to_hub("avitest-pipeline")

from transformers import pipeline

classifier_test = pipeline(model="avi30/avitest-pipeline", trust_remote_code=True)

text = 'Very GOOD!'

summary = classifier_test(text)

summary